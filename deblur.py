# -*- coding: utf-8 -*-
"""B19CSE044_B19CSE038_Mini_Project_PRML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fQPh8bK40vyU6k3eBMJDnonO0irFhxka
"""

import pandas as pd
import numpy as np

#from google.colab import drive

#drive.mount('/content/gdrive')

import cv2
import sys

'''# Opens the Video file
cap = cv2.VideoCapture('gdrive/MyDrive/Colab Notebooks/PRML_RGB_DATA/31.mp4')

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
_, img = cap.read()  
faces = face_cascade.detectMultiScale(img, 1.1, 4)  
for (x, y, w, h) in faces:
  cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)

r = max(w, h) / 2
centerx = x + w / 2
centery = y + h / 2
nx = int(centerx - r)
ny = int(centery - r)
nr = int(r * 2)
faceimg = img[ny:ny+nr, nx:nx+nr]
lastimg_low = cv2.resize(faceimg, (240, 240))
lastimg_high = cv2.resize(faceimg, (720, 720))
'''
import matplotlib.pyplot as plt

def convertToRGB(image):
    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

#plt.imshow(convertToRGB(lastimg_low))
#plt.show()
#plt.imshow(convertToRGB(lastimg_high))
#plt.show()

import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Input
from tensorflow.keras.layers import Conv2D, PReLU, Add, Dense, LeakyReLU
from tensorflow.keras.activations import tanh

import requests
import requests, zipfile, io

import time
from IPython import display

"""#Using Rosbag"""
'''
url = "https://lcas.lincoln.ac.uk/nextcloud/index.php/s/B242WHMbDlI7tnp/download"
r = requests.get(url)
z = zipfile.ZipFile(io.BytesIO(r.content))

z.extractall('/content')

!sudo apt-get install -y python-rosbag

!pip install bagpy

import bagpy
from bagpy import bagreader

b = bagreader('/content/2016-11-22-14-09-37-Joao_onlythermal.bag')

b.topic_table

import pandas as pd
LASER_MSG = b.message_by_topic('thermal_image')
LASER_MSG
#df_laser = pd.read_csv(LASER_MSG)
#df_laser

b.plot_vel(save_fig=True)

df_laser = pd.read_csv(LASER_MSG)
df_laser

vid = str(df_laser["data"][0])
vide = bytes(vid,"utf-16")
vide.decode('utf-16')
#vid.decode('mono16','strict')

import struct

print(len(vide))
s = 'b'*1690444
l = struct.unpack(s,vide)

l

"""#Reading Thermal Dataset"""

import numpy as np
data = np.array(l)
data.reshape(288,382)

import os
for dirname, _, filenames in os.walk('/content/2016-11-22-14-09-37-Joao_onlythermal'):
'''
image_list = []
for i in range(73,76,1):
  s = str(i).zfill(4)
  cap = cv2.imread('face01/'+s+".jpg")
  image_list.append(cap)

blur_list = []
for i in image_list:
  blur = cv2.GaussianBlur(i,(9,9),10)
  blur_list.append(blur)

STRIDES = (1,1)
PADDING = (1,1)
FILTER_SIZE = (3,3)
INPUT_SIZE = (240,320,3)

def resblock(y):
  shortcut = y
  y = Conv2D(filters = 64,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same")(y)
  y = PReLU()(y)
  y = Conv2D(filters = 128,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same")(y)
  y = layers.add([shortcut, y])
  return y

def make_generator_model():
  i = Input(shape=INPUT_SIZE)
  conv_1 = Conv2D(filters = 64,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same")(i)
  prelu_1 = PReLU()(conv_1)
  conv_2 = Conv2D(filters = 128,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same")(prelu_1)
  prelu_2 = PReLU()(conv_2)
  res_1 = resblock(prelu_2)
  res_2 = resblock(res_1)
  res_3 = resblock(res_2)
  res_4 = resblock(res_3)
  conv_3 = Conv2D(filters = 128,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same")(res_4)
  d = layers.add([conv_3, prelu_2])
  conv_4 = Conv2D(filters = 3,kernel_size = FILTER_SIZE,strides = (1,1), input_shape = INPUT_SIZE, padding="same", activation=tanh)(res_4)
  model = models.Model(inputs=i, outputs=conv_4)
  return model

generator = make_generator_model()
noise = tf.random.normal(shape = (2, 240, 320, 3))
generated_image = generator(noise, training=False)

import matplotlib.pyplot as plt
plt.imshow(generated_image[0, :, :, 0], cmap='gray')
plt.show()
plt.imshow(generated_image[1, :, :, 0], cmap='gray')
plt.show()

def make_discriminator_model():

  model = models.Sequential()
  #conv_block_1
  model.add(Conv2D(32, kernel_size= (3,3), strides= STRIDES, padding='same',input_shape = (240,320,3)))
  model.add(LeakyReLU())

  #conv_block_2
  model.add(Conv2D(32, kernel_size= (3,3), strides= STRIDES, padding='same'))
  model.add(LeakyReLU())

  #conv_block_3
  model.add(Conv2D(64, kernel_size= (3,3), strides= STRIDES, padding='same'))
  model.add(LeakyReLU())

  #conv_block_4
  model.add(Conv2D(128, kernel_size= (3,3), strides= STRIDES, padding='same'))
  model.add(LeakyReLU())

  #conv_block_5
  model.add(Conv2D(256, kernel_size= (3,3), strides= STRIDES, padding='same'))
  model.add(LeakyReLU())

  #conv_block_6
  model.add(Conv2D(512, kernel_size= (3,3), strides= STRIDES, padding='same'))
  model.add(LeakyReLU())

  model.add(LeakyReLU())

  model.add(Dense(1000,activation = 'sigmoid'))
  return model

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)

decision.shape

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

import os
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 50
noise_dim = 100
num_examples_to_generate = 16

# You will reuse this seed overtime (so it's easier)
# to visualize progress in the animated GIF)
seed = tf.random.normal([num_examples_to_generate, noise_dim])

@tf.function
def train_step(images, blur_image):
    noise = blur_image

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, blur_dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch, blur_batch in zip(dataset,blur_dataset):
      print(type(image_batch))
      train_step(image_batch, blur_batch)

    # Produce images for the GIF as you go
    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epoch + 1,
                             blur_batch)

    # Save the model every 15 epochs
    if (epoch + 1) % 15 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # Generate after the final epoch
  display.clear_output(wait=True)
  generate_and_save_images(generator,
                           epochs,
                           blur_batch)

def generate_and_save_images(model, epoch, test_input):
  # Notice `training` is set to False.
  # This is so all layers run in inference mode (batchnorm).
  predictions = model(test_input, training=False)

  fig = plt.figure(figsize=(4, 4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
      plt.axis('off')

  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  plt.show()

train_dataset = tf.data.Dataset.from_tensor_slices(blur_list).shuffle(100).batch(10)

main_dataset = tf.data.Dataset.from_tensor_slices(image_list).shuffle(100).batch(10)

train(main_dataset, train_dataset, 5)

'''
!sudo apt-get update
!sudo apt-get install python3-edgetpu

from edgetpu.detection.engine import DetectionEngine
from PIL import Image

# One-time initialization:
face_detector = DetectionEngine('thermal_face_automl_edge_fast_edgetpu.tflite')

thermal_cap = cv2.VideoCapture('gdrive/MyDrive/PRML_THERMAL_DATA/31.mp4')
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')
_, thermal_img = thermal_cap.read()
plt.imshow(thermal_img)
plt.show()
thermal_faces = face_cascade.detectMultiScale(thermal_img, 1.1, 4)
print(thermal_img.shape)
print(thermal_faces)
for (x, y, w, h) in thermal_faces:
  cv2.rectangle(thermal_img, (x, y), (x+w, y+h), (255, 0, 0), 2)
print(x, y, w, h)
r = max(w, h) / 2
centerx = x + w / 2
centery = y + h / 2
nx = int(centerx - r)
ny = int(centery - r)
nr = int(r * 2)
print(ny, nr, nx)
print(thermal_img.shape)
thermal_faceimg = thermal_img[ny:ny+nr, nx:nx+nr]
thermal_lastimg_low = cv2.resize(thermal_faceimg, (240, 240))
thermal_lastimg_high = cv2.resize(thermal_faceimg, (720, 720))
plt.imshow(convertToRGB(thermal_lastimg_low))
plt.show()
plt.imshow(convertToRGB(thermal_lastimg_high))
plt.show()

data = pd.read_excel("/content/PRML_VOLUNTEER_DATA.xlsx")
data.tail()

spo = list(data["Oxygen level (SpO2 content) - Pulse oximeter 1"])
label = list(data["Video Label"])
for i in range(0,len(spo)):
  if(spo[i]<90):
    print(spo[i], label[i])

import dlib
detector = dlib.get_frontal_face_detector()
# Load the predictor
predictor = dlib.shape_predictor("/content/gdrive/MyDrive/shape_predictor_68_face_landmarks.dat")

from imutils import face_utils 
lastimg_low = cv2.resize(faceimg, (240, 240))
gray = cv2.cvtColor(lastimg_high, cv2.COLOR_BGR2GRAY)
rects = detector(gray, 0)
for (i, rect) in enumerate(rects):
        # Make the prediction and transfom it to numpy array
        shape = predictor(gray, rect)
        shape = face_utils.shape_to_np(shape)
    
        # Draw on our image, all the finded cordinate points (x,y) 
        for (x, y) in shape:
            cv2.circle(lastimg_high, (x, y), 2, (0, 255, 0), -1)
    
# Show the image
plt.imshow(convertToRGB(lastimg_high))

# use the points from the shape [19:20], [23:24] for forehead
y_cord = max(shape[23:24][0][1], shape[19:20][0][1])

fore_head = lastimg_high[15:y_cord, shape[19:20][0][0]+20:shape[23:24][0][0]-20]

#plt.imshow(fore_head[:,:,0], cmap='Greys')
red_fore_head = fore_head[:,:,0]
red_fore_head = np.where(red_fore_head>=170, 0, red_fore_head)
plt.imshow(red_fore_head, cmap="Greys")
plt.imshow(convertToRGB(fore_head))



video_name = []
import os
for dirname, _, filenames in os.walk('gdrive/MyDrive/Colab Notebooks/PRML_RGB_DATA/'):
  for filename in filenames:
    video_name.append(filename)

video_name

extracted_forehead = []
video_num = []
for i in range(0, len(video_name),1):
  s = 'gdrive/MyDrive/Colab Notebooks/PRML_RGB_DATA/' + video_name[i]
  video_num.append(video_name[i][:2])
  cap = cv2.VideoCapture(s)
  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
  _, img = cap.read()  
  faces = face_cascade.detectMultiScale(img, 1.1, 4)  
  for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
  r = max(w, h) / 2
  centerx = x + w / 2
  centery = y + h / 2
  nx = int(centerx - r)
  ny = int(centery - r)
  nr = int(r * 2)
  faceimg = img[ny:ny+nr, nx:nx+nr]
  lastimg_low = cv2.resize(faceimg, (240, 240))
  lastimg_high = cv2.resize(faceimg, (1080, 1080))
  gray = cv2.cvtColor(lastimg_high, cv2.COLOR_BGR2GRAY)
  rects = detector(gray)
  for (i, rect) in enumerate(rects):
        # Make the prediction and transfom it to numpy array
        shape = predictor(gray, rect)
        shape = face_utils.shape_to_np(shape)
    
        # Draw on our image, all the finded cordinate points (x,y) 
        #for (x, y) in shape:
        #    cv2.circle(lastimg_low, (x, y), 2, (0, 255, 0), -1)
  # Show the image
  lastimg_high = convertToRGB(lastimg_high)
  y_cord = max(shape[23:24][0][1], shape[19:20][0][1])
  fore_head = lastimg_high[15:y_cord, shape[19:20][0][0]+20:shape[23:24][0][0]-20]
  #plt.imshow(fore_head)
  #plt.imshow(fore_head[:,:,0], cmap='Greys')
  red_fore_head = fore_head[:,:,0]
  red_fore_head = np.where(red_fore_head>=170, 0, red_fore_head)
  red_fore_head = cv2.resize(red_fore_head, ( 64 , 64 ))
  extracted_forehead.append(red_fore_head)

extracted_forehead = np.array(extracted_forehead)
extracted_forehead.shape

spo2 = []
for i in video_num:
  temp = int(i)
  curr_row = data.iloc[temp-2,:]
  spo2_avg = (int(curr_row[['Oxygen level (SpO2 content) - Pulse oximeter 1']]) + int(curr_row[['Oxygen level (SpO2 content) - Pulse oximeter 2']])) / 2
  spo2.append(spo2_avg)

spo2 = np.array(spo2)

flattened = []
for i in range(0, extracted_forehead.shape[0], 1):
  flattened.append((list(np.concatenate(extracted_forehead[i]).flat)))

flattened = np.array(flattened)

X = flattened/255
Y = spo2

"""# Model Creation

Train Test Split
"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)



"""## Support Vector Regressor"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svm = SVR()
svm.fit(X_train, Y_train)
pred = svm.predict(X_test)
mse = mean_squared_error(Y_test, pred)
print(mse)
plt.plot(Y_test)
plt.plot(pred)
plt.show()

weights = 100 - Y_train
svm_wt = SVR()
svm_wt.fit(X_train, Y_train, sample_weight = weights)
pred_wt = svm_wt.predict(X_test)
mse_wt = mean_squared_error(Y_test, pred_wt)
print(mse_wt)
plt.plot(Y_test)
plt.plot(pred_wt)
plt.show()

new_wt = []
for i in (Y_train):
  if (i >= 95):
    new_wt.append(1)
  elif (i >= 90 and i < 95):
    new_wt.append(2)
  elif (i >= 85 and i < 90):
    new_wt.append(5)
  else:
    new_wt.append(10)

svm_wt = SVR()
svm_wt.fit(X_train, Y_train, sample_weight = new_wt)
pred_wt = svm_wt.predict(X_test)
mse_wt = mean_squared_error(Y_test, pred_wt)
print(mse_wt)
'
plt.plot(Y_test)
plt.plot(pred_wt)
plt.show()

plt.hist(Y)



"""## Linear Regression"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train, Y_train)
lr_pred = lr.predict(X_test)


print(lr_pred)
#mse = mean_squared_error(Y_test, lr_pred)
#print(mse)
#plt.plot(Y_test)
#plt.plot(lr_pred)
#plt.show()

lr_wt = LinearRegression()
lr_wt.fit(X_train, Y_train, sample_weight = weights)
pred_lr_wt = lr_wt.predict(X_test)
mse_wt = mean_squared_error(Y_test, pred_wt)
print(mse_wt)
plt.plot(Y_test)
plt.plot(pred_lr_wt)
plt.show()

lr_wt_n = LinearRegression()
lr_wt_n.fit(X_train, Y_train, sample_weight = new_wt)
pred_lr_wt_n = lr_wt_n.predict(X_test)
mse_wt_n = mean_squared_error(Y_test, pred_lr_wt)
print(mse_wt)
plt.plot(Y_test)
plt.plot(pred_wt)
plt.show()



"""## Artificial Neural Network"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

model = keras.Sequential()
model.add(
layers.Dense(
    4096,
    activation="tanh",
    kernel_regularizer=keras.regularizers.l2(0.001),
    bias_regularizer=keras.regularizers.l2(1e-4),
    activity_regularizer=keras.regularizers.l2(1e-5),
)
)

model.add(layers.BatchNormalization())
model.add(layers.LeakyReLU())

model.add(
layers.Dense(
      512,
      activation="tanh",
      kernel_regularizer=keras.regularizers.l2(1e-4),
      bias_regularizer=keras.regularizers.l2(1e-4),
      activity_regularizer=keras.regularizers.l2(1e-5),
  )
)

model.add(layers.BatchNormalization())
model.add(layers.LeakyReLU())

layers.Dense(
      128,
      activation="tanh",
      kernel_regularizer=keras.regularizers.l2(1e-4),
      bias_regularizer=keras.regularizers.l2(1e-4),
      activity_regularizer=keras.regularizers.l2(1e-5),
  )

model.add(layers.Dense(1))

model.compile(
  optimizer="adam",
  loss=tf.keras.losses.MeanSquaredError(),
  metrics=["accuracy"],
)

model.fit(X_train,Y_train, epochs=200)

model = keras.Sequential([
      layers.Dense(4096, activation='relu'),
      layers.Dense(1024, activation='relu'),
      layers.Dense(512, activation='relu'),
      layers.Dense(256, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(32, activation='relu'),
      layers.Dense(1)
  ])

model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))

model.fit(X_train, Y_train, epochs=100)

pred_ann = model.predict(X_test)
mse_ann = mean_squared_error(pred_ann, Y_test)
mse_ann

plt.plot(pred_ann)
plt.plot(Y_test)
plt.show()

model.summary()

from sklearn.linear_model import BayesianRidge

br = BayesianRidge()
br.fit(X_train, Y_train)
br_pred = br.predict(X_test)

mse_br = mean_squared_error(br_pred, Y_test)
print(mse_br)

plt.plot(Y_test)
plt.plot(br_pred)
plt.show()

br_wt = BayesianRidge()
br_wt.fit(X_train, Y_train, sample_weight=weights)
br_wt_pred = br_wt.predict(X_test)

mse_br_wt = mean_squared_error(br_wt_pred, Y_test)
print(mse_br_wt)

plt.plot(Y_test)
plt.plot(br_wt_pred)
plt.show()

br_wt_n = BayesianRidge()
br_wt_n.fit(X_train, Y_train, sample_weight=new_wt)
br_wt_n_pred = br_wt_n.predict(X_test)

mse_br_wt_n = mean_squared_error(br_wt_n_pred, Y_test)
print(mse_br_wt_n)

plt.plot(Y_test)
plt.plot(br_wt_n_pred)
plt.show()
'''
